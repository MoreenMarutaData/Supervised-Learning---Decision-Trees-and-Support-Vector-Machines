# -*- coding: utf-8 -*-
"""HYPERTHYROID PROBABILITY PREDICTION MODEL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z223A5DZ1gakrAcV8VA0LpCIirNasl70

## DEFINE THE QUESTION

Build a model that determines whether or not the patient's symptoms indicate that the patient has hypothyroid.

## METRIC FOR SUCCESS

To find the best model that analyses symptoms and predicts the probability of a human having hypothyroid infection.

##UNDERSTANDING THE CONTEXT

There are various models used in machine learning. Depending on the features, accuracy, RMSE, R Score and MSE, a good classifier model should be choosen, so it is easier to predict the probability of a patient having hypothyrid complications.

## EXPERIMENTAL DESIGN

Reading the Data

PrepaRing and cleaning the Data

Exploratory Data Analysis

Then modelling using the following approaches:

* Approach 0: Random forest Regressor
* Approach 1: Random forest classifier
* Approach 2: Ada boost
* Approach 3: Gradient boost
* Approach 4: SVM - linear
* Approach 5: SVM - Radial Basis Function
* Approach 6: SVM - polynomial

## LIBRARIES
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import StrMethodFormatter
# %matplotlib inline

from sklearn.model_selection import train_test_split,GridSearchCV, RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn import metrics
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import StandardScaler

from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
from IPython.display import Image  
import pydotplus
import pandas_profiling as pp

import warnings
warnings.filterwarnings('ignore')

"""## READING AND CHECKING DATA"""

df=pd.read_csv('/content/hypothyroid.csv')
df.head()

df.tail()

"""Dataset Columns
* Status
* Age
* Sex
* on_thyroxine
* query_on_thyroxine
* on_antithyroid_medication
* thyroid_surgery
* query_hypothyroid
* query_hyperthyroid
* pregnant
* sick
* tumor
* lithium
* goitre
* TSH_measured
* TSH
* T3_measured
* T3
* TT4_measured
* TT4
* T4U_measured	
* T4U	FTI_measured	
* FTI	
* TBG_measured	
* TBG
"""

df.info()

df.dtypes

df.columns

df.duplicated().sum()

"""## CLEANING DATA"""

df=df.drop_duplicates(keep=False)
df.duplicated().sum()

df.shape

# Get unique values for each column
column_names = df.columns.to_list()

for col in column_names:
  print(f'Column: {col} \n')
  print(df[col].unique())
  print(' \n \n')

"""The sex column has a unique value '?' so lets replace it with 'other'"""

df['sex'].replace({'?': 'other'},inplace=True)
df['sex'].unique()

"""some other columns also have '?', lets replace it with null values then zeros so we cannot lose the data"""

df.replace('?', np.nan, inplace=True)

df=df.fillna(0)

column_names = df.columns.to_list()

for col in column_names:
  print(f'Column: {col} \n')
  print(df[col].unique())
  print(' \n \n')

"""check for missing values"""

df.isnull().sum().sort_values(ascending=False) / df.shape[0] * 100

"""Some numerical columns are  identified as object so lets change their data types and check to ensure we can see the changes"""

df.head()

# df['age','TSH','T3','TT4','T4U','FTI','TBG']=pd.to_numeric(df['age','TSH','T3','TT4','T4U','FTI','TBG'], downcast='float')
num=['age','TSH','T3','TT4','T4U','FTI','TBG']
#df[num] = df[num].astype(np.float64)

for col in df.columns:
  if col in num:
    df[col]= pd.to_numeric(df[col])
  
  else:
    df[col] = df[col].astype('category')

df.dtypes

"""checking for outliers. """

fig, axes = plt.subplots(nrows = 3, ncols = 2, figsize = (20, 20))
# label the title
fig.suptitle('Box plots for Dataset features', y= 1, color = 'black', fontsize = 15)
sns.set(style = 'whitegrid', context = 'notebook')
# Use a for loop to iterate through our num list
for ax, data, name in zip(axes.flatten(), df, num):
  sns.boxplot(df[name], ax = ax)
  ax.set_title('Box plot for '+ name)  
plt.show()

"""There is a lot of outliers except for the age column. Although outliers affect analysis, this is for medical purposes and removing a lot of data might negate our results.

## EDA
"""

sns.countplot(x='status',data=df)
plt.title('Status of Patients')
plt.show()

sns.countplot(x='status', hue= "sex",data=df)
plt.title('Status of Patients')
plt.show()

sns.countplot(x='status', hue= "tumor",data=df)
plt.title('Status of Patients with tumor')

sns.countplot(x='status', hue= "thyroid_surgery",data=df)
plt.title('Status of Patients with thyroid_surgery')
plt.show()

sns.countplot(x='status', hue= "goitre",data=df)
plt.title('Status of Patients with goitre')
plt.show()

sns.countplot(x='status', hue= "on_antithyroid_medication",data=df)
plt.title('Status of Patients with on_antithyroid_medication')
plt.show()

sns.countplot(x='status', hue= "lithium",data=df)
plt.title('Status of Patients with lithium')
plt.show()

df.columns

sns.countplot(x='status', hue= "query_hypothyroid",data=df)
plt.title('Status of Patients with lithium')
plt.show()

col_names = ["age", "TSH", "T3", "TT4", "T4U", "FTI"]

fig, ax = plt.subplots(len(col_names), figsize = (15,35))
for i, col_val in enumerate(col_names):

    sns.lineplot(x = df[col_val], y=df['status'], ax=ax[i])
    ax[i].set_title('A linegraph of the relationship between {} and status'.format(col_val), fontsize=20)
    ax[i].set_xlabel(col_val, fontsize=20)
    ax[i].figure.tight_layout(pad=3.0)
plt.show()

df.columns

numerical=df[['TSH_measured', 'TSH', 'T3_measured', 'T3', 'TT4_measured', 'TT4','T4U_measured', 'T4U', 'FTI_measured', 'FTI', 'TBG_measured', 'TBG']]
numerical.corr()

sns.heatmap(numerical.corr())

"""The features have very low correlations

## MODELLING

### PREPROCESSING
"""

from sklearn.preprocessing import LabelEncoder

hp=df.apply(LabelEncoder().fit_transform)

"""On the sex column, 1 is male, 0 is female and 2 is other. The columns that had 't' and 'f', f is o and t is 1. The columns that had 'y' and 'n', y is 1 and n is 0."""

hp.dtypes

"""Since we are building tree based algorithms, we will not sale our features.

### PART 1 DECISION TREES

RANDOM FOREST

lets use a forest regressor
"""

# lets assign the dependent and independent variables

y = hp['status']

features = hp.columns.to_list()
features.remove('status')

X = hp[features]

# Train using 80% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Import the model
from sklearn.ensemble import RandomForestRegressor

forest = RandomForestRegressor(n_estimators = 100, random_state=50,max_depth=5)
forest = forest.fit(X_train, y_train)

# Predict based on the model we've trained
y_pred = forest.predict(X_test)


comparison_frame = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})

comparison_frame.describe()

"""lets check the errors"""

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

"""This looks a bit overfitted, which means our test model is performing very well.

plotting the random forest
"""

dot_data = StringIO()

# picks a specific tree from the forest
tree = forest.estimators_[50]

export_graphviz(tree, out_file = dot_data, filled = True, rounded = False,
                special_characters = True, feature_names = features)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('hypothyroid.png')
Image(graph.create_png())

"""Feature importance shows us the most relevant features. We could challenge the solution by using the necessary relevant features and observe our findings."""

feature_importance=pd.Series(forest.feature_importances_,index=features)
plt.figure(figsize=(12, 10))
feature_importance.plot(kind='barh')
plt.title('Feature importance')

"""lets use a classifier and observe the findings"""

y = hp['status']

features = hp.columns.to_list()
features.remove('status')

X = hp[features]

# Train using 80% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Import the model
from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(n_estimators = 100, random_state=50,max_depth=5)
forest = forest.fit(X_train, y_train)

# Predict based on the model we've trained
y_pred = forest.predict(X_test)


# Plot confusion matrix
matrix = pd.DataFrame(confusion_matrix(y_test, y_pred), index=[0,1], columns=[0,1])
sns.heatmap(matrix, annot=True)

comparison_frame = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(comparison_frame.describe())

print(confusion_matrix(y_test, y_pred))
print( 'Accuracy Score =',accuracy_score(y_test, y_pred))

"""This a better accuracy score"""

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

dot_data = StringIO()

# picks a specific tree from the forest
tree = forest.estimators_[50]

export_graphviz(tree, out_file = dot_data, filled = True, rounded = False,
                special_characters = True, feature_names = features)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('hypothyroid.png')
Image(graph.create_png())

feature_importance=pd.Series(forest.feature_importances_,index=features)
plt.figure(figsize=(12, 10))
feature_importance.plot(kind='barh')
plt.title('Feature Importance')

"""lets print feature importances in a descending order"""

random_forest_features = pd.DataFrame({'Features': X.columns.to_list(), 'Importance':feature_importance })
random_forest_features.sort_values('Importance', ascending=False)

"""ADA BOOSTING"""

from sklearn.ensemble import AdaBoostClassifier

# create our classifier with 200 trees of depth 1
ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1), n_estimators = 200)

# fitting our model to our training data
ada.fit(X_train, y_train)

# making predictions
ada_y_pred = ada.predict(X_test)

# compare actual values to predicted values
ada_compare = pd.DataFrame({'Actual' : y_test, 'Predicted' : ada_y_pred})

# preview our summary statistics
ada_compare.describe()

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, ada_y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, ada_y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ada_y_pred)))

print(confusion_matrix(y_test, ada_y_pred))
print( 'Accuracy Score =',accuracy_score(y_test, ada_y_pred))

"""the accuracy score has reduced a bit.

GRADIENT BOOSTING
"""

from sklearn.ensemble import GradientBoostingClassifier
target_number_of_trees = 100

gbr = GradientBoostingClassifier(n_estimators=target_number_of_trees, learning_rate=0.1, max_depth=1)
gbr = gbr.fit(X_train, y_train)

# predict using the model
y_pred_gbr = gbr.predict(X_test)


# compare actual values to predicted values
comparison = pd.DataFrame({'Actual' : y_test, 'Predicted' : y_pred_gbr})
comparison

# check the accuracy
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_gbr))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_gbr))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_gbr)))

print(confusion_matrix(y_test, y_pred_gbr))
print( 'Accuracy Score =',accuracy_score(y_test, y_pred_gbr))

"""Gradient boosted trees have a higher accuracy than ada boosted trees and random forest classified trees for this dataset."""

feature_importance=pd.Series(forest.feature_importances_,index=features)
plt.figure(figsize=(12, 10))
feature_importance.plot(kind='barh')
plt.title('Feature Importance')

"""### PART 2 SUPPORT VECTOR MACHINES

From boosting our models, the best features are FTI and TSH. Both compound for over 50% of the reuslts.
"""

X= hp[['FTI','TSH']]
y=hp['status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

"""lets plot both of them to see how they compare"""

sns.scatterplot('FTI','TSH', data=df, hue='status', palette='Set2')
plt.show()

"""LINEAR KERNEL MODEL"""

# Build the svm model 
model = SVC(kernel = 'linear')
# Train the model using the training set
model.fit(X_train,y_train)
# Predict the response for the test set
y_pred_l = model.predict(X_test)

# check the accuracy
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_l))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_l))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_l)))

print(confusion_matrix(y_test, y_pred_l))
print( 'Accuracy Score =',accuracy_score(y_test, y_pred_l))
print(classification_report(y_test,y_pred_l))

"""RBF"""

rbf = SVC(kernel = 'rbf', gamma = 'auto')

# train the model
rbf.fit(X_train, y_train)

# make the prediction
rbf_y_pred = rbf.predict(X_test)

#check the accuracy
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, rbf_y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, rbf_y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, rbf_y_pred)))

print(confusion_matrix(y_test, rbf_y_pred))
print( 'Accuracy Score =',accuracy_score(y_test, rbf_y_pred))
print(classification_report(y_test,rbf_y_pred))

"""POLYNOMIAL"""

poly = SVC(kernel = 'poly', gamma = 'auto')

# train the model
poly.fit(X_train, y_train)

# make the prediction
poly_y_pred = poly.predict(X_test)

#check the accuracy
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, poly_y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, poly_y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, poly_y_pred)))

print(confusion_matrix(y_test, poly_y_pred))
print( 'Accuracy Score =',accuracy_score(y_test, poly_y_pred))
print(classification_report(y_test,poly_y_pred))

"""Poly was the slowest perfoming kernel. RBF had the least accuracy score. Linear ad Poly kernels have high accuracy scores.

RECOMMENDATIONS.

I could have done some PCA to improve my results.

the dataset is imbalanced meaning f1 score would be the best predictor for this case.

now lets use the linear kernel for the whole dataset and observe
"""

y = hp['status']

features = hp.columns.to_list()
features.remove('status')

X = hp[features]

# Train using 80% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Build the svm model 
model = SVC(kernel = 'linear')
# Train the model using the training set
model.fit(X_train,y_train)
# Predict the response for the test set
y_pred_l = model.predict(X_test)


# compare actual values to predicted values
comparison = pd.DataFrame({'Actual' : y_test, 'Predicted' : y_pred_l})
comparison

# check the accuracy
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_l))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_l))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_l)))

print(confusion_matrix(y_test, y_pred_l))
print( 'Accuracy Score =',accuracy_score(y_test, y_pred_l))
print(classification_report(y_test,y_pred_l))

"""the accuracy is lower by 0.001 but still performs well. f1 scores are great too."""

